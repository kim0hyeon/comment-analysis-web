# Copyright [2025-05-28] [monologg]

# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at

#     http://www.apache.org/licenses/LICENSE-2.0

# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import streamlit as st
from googleapiclient.discovery import build
import urllib.parse as urlparse
import pandas as pd
import altair as alt
import onnxruntime as ort
from transformers import AutoTokenizer
import numpy as np
import os
from googleapiclient.errors import HttpError
import time
import random
import json
from azure.storage.blob import BlobServiceClient
from pathlib import Path

MODEL_PATH = Path("models/koelectra.onnx")
CONTAINER_NAME = "models"
BLOB_NAME = "koelectra.onnx"

def download_model_from_blob():
    conn_str = os.environ["AZURE_STORAGE_CONNECTION_STRING"]
    service = BlobServiceClient.from_connection_string(conn_str)
    container = service.get_container_client(CONTAINER_NAME)
    MODEL_PATH.parent.mkdir(exist_ok=True)
    with open(MODEL_PATH, "wb") as f:
        f.write(container.get_blob_client(BLOB_NAME).download_blob().readall())

# Download the model if not already present
if not MODEL_PATH.exists():
    download_model_from_blob()

# ë§Œì•½ ëª¨ë¸ì˜ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ë‹¤ë©´ Blobì—ì„œ ë‹¤ì‹œ ë‚´ë ¤ë°›ëŠ”ë‹¤.
def should_download():
    return not MODEL_PATH.exists() or MODEL_PATH.stat().st_size < 1_000_000

if should_download():
    download_model_from_blob()

@st.cache_resource
def load_tokenizer_and_session():
    tokenizer = AutoTokenizer.from_pretrained(
        "Copycats/koelectra-base-v3-generalized-sentiment-analysis"
    )
    session = ort.InferenceSession(os.path.join("models", "koelectra.onnx"))
    return tokenizer, session

tokenizer, ort_session = load_tokenizer_and_session()

def analyze_sentiment(text):
    enc = tokenizer(
        text,
        return_tensors="np",
        padding=True,
        truncation=True,
        max_length=128
    )
    logits = ort_session.run(
        None,
        {"input_ids": enc["input_ids"], "attention_mask": enc["attention_mask"]}
    )[0]
    probs = np.exp(logits) / np.exp(logits).sum(axis=1, keepdims=True)
    neg, pos = probs[0]
    prob = max(neg, pos) * 100
    if neg > pos and neg > 0.75:
        label = "ë¶€ì •"
    elif pos >= neg:
        label = "ê¸ì •"
    else:
        label = "ì¤‘ë¦½"
    return label, prob


##################################
#  Youtube API ê´€ë ¨
##################################
API_key = "AIzaSyBrafWWm7JMMHCSe4SpB6vfqfx4YroAOoM"  # ì‹¤ì œ ê°’ìœ¼ë¡œ êµì²´
youtube = build("youtube", "v3", developerKey=API_key)

def get_video_id_from_url(url: str):
    parsed_url = urlparse.urlparse(url)
    query_dict = urlparse.parse_qs(parsed_url.query)
    video_id_list = query_dict.get("v")
    if video_id_list and len(video_id_list) > 0:
        return video_id_list[0]
    return None

def get_comments_by_video_id(video_id, max_results=50):
    comments = []
    page_token = None
    while True:
        # Build request parameters
        params = {
            "part": "snippet,replies",
            "videoId": video_id,
            "maxResults": max_results,
            "textFormat": "plainText",
            "order": "time"
        }
        if page_token:
            params["pageToken"] = page_token

        # Retry loop for transient errors
        for attempt in range(3):
            try:
                request = youtube.commentThreads().list(**params)
                response = request.execute()
                break
            except HttpError as e:
                # Parse error reason
                try:
                    err = json.loads(e.content.decode())
                    reason = err["error"]["errors"][0].get("reason", "")
                except:
                    raise
                if reason == "processingFailure":
                    backoff = (2 ** attempt) + random.random()
                    time.sleep(backoff)
                    continue
                else:
                    raise
        else:
            raise RuntimeError("ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ë°˜ë³µ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")

        # Extract comments
        for item in response.get("items", []):
            top_comment = item["snippet"]["topLevelComment"]["snippet"]["textDisplay"]
            comments.append(top_comment)
            if "replies" in item:
                for reply_item in item["replies"]["comments"]:
                    comments.append(reply_item["snippet"]["textDisplay"])

        # Next page or break
        page_token = response.get("nextPageToken")
        if not page_token:
            break

    return comments

def run_youtube_analysis():
    st.title("ğŸ“‘ ìœ íŠœë¸Œ ì˜ìƒ ëŒ“ê¸€ ê°ì • ë¶„ì„")

    # ì‚¬ìš©ë°©ë²• ì„¤ëª…í•˜ëŠ” ì‚¬ì´ë“œë°”
    with st.sidebar:
        st.subheader("ì‚¬ìš© ë°©ë²•")
        sidebar_url = st.text_area("", value="ì—¬ê¸°ì— ìœ íŠœë¸Œ urlì„ ì…ë ¥í•˜ì„¸ìš”!")

        # ë¶„ì„í•˜ê¸° ë²„íŠ¼ & ì˜¤ë¥¸ìª½ì— "ğŸ‘ˆí´ë¦­!"
        col_sb1, col_sb2 = st.columns([3, 1])
        with col_sb1:
            sidebar_btn1 = st.button("'ìœ íŠœë¸Œ ëŒ“ê¸€ ì¶”ì¶œ & ê°ì • ë¶„ì„'")
        with col_sb2:
            st.write("ğŸ‘ˆ í´ë¦­!")

        # ë¶„ì„ ê²°ê³¼
        st.markdown("---")
        st.subheader("ë¶„ì„ ê²°ê³¼ (ì˜ˆì‹œ)")
        st.image("img/page2_result_count.png")
        st.image("img/page2_result_chart.png")
        st.image("img/page2_result_text.png")

        # ì´ˆê¸°í™” ë°©ë²• í‘œì‹œ
        st.markdown("---")
        st.subheader("ë‹¤ì‹œ ì‘ì„±í•˜ê³ ì‹¶ë‹¤ë©´?")
        col_sb3, col_sb4 = st.columns([1, 2])
        with col_sb3:
            sidebar_btn2 = st.button("'ì´ˆê¸°í™”'")
        with col_sb4:
            st.write("ğŸ‘ˆ í´ë¦­!")


    youtube_url = st.text_input("""ë¶„ì„í•  YouTube ë§í¬ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆì‹œ)\n
                                https://www.youtube.com/watch?v=fvaJDMD5xSk\n
                                """)

    # ë²„íŠ¼ ë ˆì´ì•„ì›ƒ (ë¶„ì„í•˜ê¸° + ì´ˆê¸°í™” ë²„íŠ¼ ë‚˜ë€íˆ)
    col1, col2, col3 = st.columns([3, 3, 1])

    with col1:
        if st.button("ìœ íŠœë¸Œ ëŒ“ê¸€ ì¶”ì¶œ & ê°ì • ë¶„ì„"):
            if not youtube_url.strip():
                st.warning("YouTube ë§í¬ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                video_id = get_video_id_from_url(youtube_url)
                if not video_id:
                    st.error("ìœ íš¨í•œ ìœ íŠœë¸Œ URLì´ ì•„ë‹™ë‹ˆë‹¤. v= íŒŒë¼ë¯¸í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                else:
                    st.session_state["YouTube_result"] = {
                        "video_id": video_id
                    }
                    st.session_state["youtube_url"] = youtube_url
                    st.session_state["popup_shown"] = False  # íŒì—… ì´ˆê¸°í™”

    with col3:
        if st.button("ì´ˆê¸°í™”"):
            st.session_state["youtube_url"] = ""
            st.session_state.pop("YouTube_result", None)
            st.rerun()

    if "YouTube_result" in st.session_state:
        with st.spinner("ëŒ“ê¸€ì„ ê°€ì ¸ì˜¤ê³  ìˆìŠµë‹ˆë‹¤..."):
            result = st.session_state["YouTube_result"]
            video_id = result["video_id"]
            comments = get_comments_by_video_id(video_id, max_results=50)
        
            if len(comments) == 0:
                st.warning("ëŒ“ê¸€ì´ ì—†ê±°ë‚˜, ëŒ“ê¸€ ì„¤ì •ì´ ë¹„í™œì„±í™”ëœ ì˜ìƒì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                st.success(f"ì´ {len(comments)}ê°œì˜ ëŒ“ê¸€ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.")
                
                sentiments_result = []
                with st.spinner("ëŒ“ê¸€ ê°ì • ë¶„ì„ì¤‘..."):
                        for cmt in comments:
                            s_label, s_score = analyze_sentiment(cmt)
                            sentiments_result.append((cmt, s_label, s_score))
                
                # í†µê³„
                total_count = sum(1 for _, label, _ in sentiments_result)
                pos_count = sum(1 for _, label, _ in sentiments_result if label == "ê¸ì •")
                neg_count = sum(1 for _, label, _ in sentiments_result if label == "ë¶€ì •")
                neu_count = sum(1 for _, label, _ in sentiments_result if label == "ì¤‘ë¦½")

                if neg_count/total_count*100 >= 15:
                    st.error("ì‹œì²­ì— ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤!")
                else:
                    st.info("ì‹œì²­ì— ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤!")

                st.markdown(
                    "<h3 style='margin-top:0;'>ì°¨íŠ¸</h3>",
                    unsafe_allow_html=True
                )
                                
                # ì°¨íŠ¸ ì‹œê°í™”
                df = pd.DataFrame({
                    "Sentiment": ["Positive", "Negative", "Neutral"],
                    "Percent": [
                        float(f"{(pos_count/total_count * 100):.2f}"),
                        float(f"{(neg_count/total_count * 100):.2f}"),
                        float(f"{(neu_count/total_count * 100):.2f}")
                    ]
                })

                chart = (
                    alt.Chart(df)
                    .mark_arc(stroke="black", strokeWidth=2)  # í…Œë‘ë¦¬ë¥¼ ê²€ì •ìƒ‰, ë‘ê»˜ 2ë¡œ ì„¤ì •
                    .encode(
                        theta=alt.Theta(field="Percent", type="quantitative"),
                        color=alt.Color(
                            field="Sentiment",
                            type="nominal",
                            scale=alt.Scale(
                                domain=["Positive", "Negative", "Neutral"],
                                range=["#ADD8E6", "#FFA07A", "gray"]  # ì—°í•œ íŒŒë€ìƒ‰, ì—°í•œ ë¹¨ê°„ìƒ‰, íšŒìƒ‰
                            ),
                            legend=alt.Legend(labelColor="black", titleColor="black")  # ë²”ë¡€ í…ìŠ¤íŠ¸ë¥¼ ì§„í•˜ê²Œ ì„¤ì •
                        ),
                        tooltip=["Sentiment", "Percent"]
                    )
                    .properties(width=400, height=400)
                )

                st.altair_chart(chart, use_container_width=True)
                
                # ë¶„ì„ ê²°ê³¼ ë°•ìŠ¤ (í•œ ë²ˆì˜ HTML ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ ë Œë”ë§)
                html_summary = (
                    "<div style='border:1px solid #ccc; padding:15px; border-radius:8px; "
                    "background-color:#fafafa;'>"
                    "<h3 style='margin-top:0;'>ë¶„ì„ ê²°ê³¼</h3>"
                    f"<p style='margin:4px 0;'><strong>ì´ ëŒ“ê¸€ ìˆ˜: {total_count}ê°œ</strong></p>"
                    f"<p style='margin:4px 0;'><span style='color:blue; font-weight:bold;'>ê¸ì •</span> "
                    f"ì ì¸ ëŒ“ê¸€: {pos_count}ê°œ</p>"
                    f"<p style='margin:4px 0;'><span style='color:red; font-weight:bold;'>ë¶€ì •</span> "
                    f"ì ì¸ ëŒ“ê¸€: {neg_count}ê°œ</p>"
                    f"<p style='margin:4px 0;'><span style='font-weight:bold;'>ì¤‘ë¦½</span> "
                    f"ì ì¸ ëŒ“ê¸€: {neu_count}ê°œ</p>"
                    "</div>"
                    "<br>"
                    "<br>"
                )
                st.markdown(html_summary, unsafe_allow_html=True)
                
                # ì˜ˆì‹œ ëŒ“ê¸€ ì¶œë ¥ (HTML ë°•ìŠ¤ ì „ì²´ ìƒì„± í›„ í•œ ë²ˆì— ë Œë”ë§)
                html_comments = "<div style='border:1px solid #ccc; padding:15px; border-radius:8px; background-color:#f9f9f9;'>"
                html_comments += "<h3 style='margin-top:0;'>ì˜ˆì‹œ ëŒ“ê¸€</h3>"
                for i, (comment_text, label, s_score) in enumerate(sentiments_result[:10], start=1):
                    # ëŒ“ê¸€ ë³¸ë¬¸
                    html_comments += f"<p style='margin-bottom:4px;'><strong>{i}. {comment_text}</strong></p>"
                    # ê°ì • ë ˆì´ë¸”
                    color = 'blue' if label == 'ê¸ì •' else 'red' if label == 'ë¶€ì •' else 'gray'
                    html_comments += f"<p style='margin-top:0; margin-bottom:8px;'><span style='color:{color}; font-weight:bold;'>{label}</span> (s_score: {s_score:.2f}%)</p>"
                    html_comments += "<br>"
                html_comments += "</div>"
                st.markdown(html_comments, unsafe_allow_html=True)
                        

if __name__ == "__main__":
    run_youtube_analysis()